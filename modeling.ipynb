{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Feb 26 16:10:50 2018\n",
    "\n",
    "@author: HP\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 13\n",
    "iterations = 100000\n",
    "numDimensions = 100\n",
    "maxSeqLength = 100\n",
    "\n",
    "options = {\n",
    "           'drug' :            [1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "           'hang' :            [0,1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "           'gun' :             [0,0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "           'alcohol' :         [0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "           'cut' :             [0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "           'jump' :            [0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "           'disease' :         [0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "           'bleed' :           [0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "           'drown' :           [0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "           'vehicle' :         [0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "           'carbon monoxide' : [0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "           'starve' :          [0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "           'electrocute' :     [0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "           }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "########################################################################\n",
    "def getLabel(S_method):\n",
    "    #[drug,hang,gun,alcohol,cut,jump,disease,bleed,drown,vehicle,carbon monoxide,starve,electrocute]    \n",
    "   \n",
    "    return options[S_method]\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    glove2word2vec(glove_input_file = gloveFile, word2vec_output_file=\"gensim_glove_vectors_100.txt\")\n",
    "    glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors_100.txt\", binary=False)\n",
    "#    print(\"Loading Glove Model\")\n",
    "#    f = open(gloveFile,'r')\n",
    "#    model = {}\n",
    "#    for line in f:\n",
    "#        splitLine = line.split()\n",
    "#        word = splitLine[0]\n",
    "#        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "#        model[word] = embedding\n",
    "#        print(word)\n",
    "#    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return glove_model\n",
    "##############################################################################\n",
    "    \n",
    "def convert_data_to_index(string_data, wv):\n",
    "    index_data = []\n",
    "    for word in string_data:\n",
    "        if word in wv:\n",
    "            index_data.append(wv[word].index)\n",
    "    return index_data\n",
    "###################################################################\n",
    "    \n",
    "def convertModelIntoEmbdMtrx(model,vector_dim):\n",
    "    embedding_matrix = np.zeros((len(model.vocab), vector_dim),dtype=np.float32)\n",
    "    for i in range(len(model.vocab)):\n",
    "        embedding_vector = model.word_vec(model.index2word[i])\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def splitTrainTest(frame):\n",
    "    df_test = pd.DataFrame()\n",
    "    df_train = pd.DataFrame()\n",
    "    for k,v in options.items():\n",
    "        tempbool = frame['S_method'] == k\n",
    "        tempFrame = frame[tempbool]\n",
    "        length = len(tempFrame)\n",
    "        train, test = np.split(tempFrame, [int(.7*length)])    \n",
    "        df_train = df_train.append(train)\n",
    "        df_test = df_test.append(test)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([len(tndf.index), maxSeqLength])\n",
    "    for i in range(len(tndf.index)):\n",
    "        id_idx = tndf.index[i]\n",
    "#        print(\"train Batch : \" + str(id_idx))\n",
    "        arr[i] = ids[id_idx]\n",
    "        labels.append(tndf.label[tndf.index[i]])\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([len(tsdf.index), maxSeqLength])\n",
    "    for i in range(len(tsdf.index)):\n",
    "        id_idx = tsdf.index[i]\n",
    "        arr[i] = ids[id_idx]\n",
    "        labels.append(tsdf.label[tsdf.index[i]])\n",
    "    return arr, labels\n",
    "\n",
    "rows = []\n",
    "#csv_header = ['user', 'item', 'time', 'rating', 'review']\n",
    "#frame_header = ['user', 'item', 'rating', 'review']\n",
    "\n",
    "wordModel = loadGloveModel('../../Pretrained_WordVectors/glove.twitter.27B/glove.twitter.27B.100d.txt')\n",
    "WVDic = wordModel.vocab\n",
    "idx = convert_data_to_index(\"test string data\".split(),wordModel.vocab)\n",
    "wordVectors = convertModelIntoEmbdMtrx(wordModel,100)\n",
    "\n",
    "ids = np.zeros((503, maxSeqLength), dtype='int32')\n",
    "cmntCntr = 0\n",
    "with open('Manual_Annotations_Sent.csv', 'rt') as f_input:\n",
    "    for row in csv.DictReader(f_input, delimiter='|', fieldnames=['sent','S_method'], skipinitialspace=True):\n",
    "        cleanedSent = cleanSentences(row['sent'])\n",
    "        split = cleanedSent.split()\n",
    "        wordCount = len(split)\n",
    "        if wordCount <= maxSeqLength:\n",
    "            idstemp = convert_data_to_index(split,wordModel.vocab)\n",
    "            ids[cmntCntr][0:len(idstemp)] = idstemp\n",
    "            rows.append([cleanedSent, getLabel(row['S_method']), row['S_method'], wordCount])\n",
    "            cmntCntr = cmntCntr + 1\n",
    "        \n",
    "frame = pd.DataFrame(rows, columns=['sent','label','S_method','word_count'])\n",
    "tndf, tsdf = splitTrainTest(frame)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [len(tndf.index), numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [len(tndf.index), maxSeqLength])\n",
    "\n",
    "data = tf.Variable(tf.zeros([len(tndf.index), maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"./tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(\"iteration \" + str(i))\n",
    "     #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "#Save the network every 10,000 training iterations\n",
    "    if (i % 1000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
